##CHAPTER 4 모델 훈련
###4.1 선형 회귀
* 식 4-1 선형 회귀 모델의 예측
* 식 4-2 선형 회귀 모델의 예측(벡터 형태)
* RMSE를 최소화하는 Θ를 찾아야 함
* RMSE보다 평균 제곱 오차(MSE)를 더 선호함
  - 식 4-3 선형 회귀 모델의 MSE 비용 함수
  - MSE(X,hΘ)를 MSE(Θ)로도 표시 
###4.1.1 정규방정식
* Θ의 값을 얻을 수 있는 수학 공식이 있음
  - 식 4-4 정규방정식
###4.1.2 계산 복잡도
* O(n^2.4) ~ O(n^3)의 계산 복잡도를 가짐(n은 특성 수)
  - 특성수가 두배로 늘어나면 계산시간은 5.3~8배 증가
  - 특성수가 적은 샘플에 적합
* 훈련 세트의 샘플 수에는 선형적으로 증가
  - 메모리 공간이 허락된다면 큰 훈련 세트도 효율적으로 처리

###4.2 경사 하강법
* 경사 하강법(Gradient Descent:GD)은 비용함수를 최소화하기 위해 반복해서 파라미터를 조정
* 그림 4-3 경사 하강법
* 학습률(learning rate) 하이퍼파라미터
  - 학습률이 너무 작으면 시간이 오래 걸림
    - 그림 4-4 학습률이 너무 작을 때
  - 학습률이 너무 크면 큰 값으로 발산되어 해를 찾지 못함
    - 그림 4-5 학습률이 너무 클 때
* 그림 4-6 경사 하강법의 문제점
  - 지역 최소값(local minimum)에 수렴할 수 있음
  - 평탄한 지역에서 멈출수 있음
* 선형 회귀를 위한 MSE 비용 함수는 볼록 함수(convex function)
  - 지역 최소값이 없고 전역 최소값(global minimum)만 존재함
  - 연속된 함수
  - 기울기가 갑자기 변하지 않음
* 그림 4-7 특성 스케일에 따른 경사 하강법
  - 특성들의 스케일링 필요
  - 사이킷런의 StandardScaler를 사용
####4.2.1 배치 경사 하강법
* 편도 함수(partial derivative)
  - Θj가 조금 변경될 때 비용 함수가 얼마나 바뀌는가를 나타내는 함수
  - 식 4-5 비용 함수의 편도함수
  - 식 4-6 비용 함수의 그래디언트 벡터
    - 편도 함수를 사용하여 한꺼번에 계산
* 식 4-7 경사 하강법의 스텝
  - 학습률 Π 사용
  - 그림 4-8 여러 가지 학습률에 대한 경사 하강법
  - 적절한 학습률을 찾으려면 그리드 탐색 사용
    - 수렴하는데 너무 오래 걸리는 것을 막기 위해 반복 횟수 제한 필요
    - 반복 횟수 지정은?
      - 반복 횟수를 크게 지정하고 허용오차보다 작아지면 알고리즘 중지
      
####4.2.2 확률적 경사 하강법
* 배치 경사 하강법의 문제
  - 매 스텝에서 전체 훈련 세트를 사용해 그래디언트를 계산
* 확률적 경사 하강법
  - 매 스텝에서 딱 한 개의 샘플을 무작위로 선택, 그래디언트를 계산
  - 매 반복에서 매우 적은 데이터만 처리하기 때문에 빠름
  - 배치 경사 하강법보다 불안정
  - 불규칙성은 지역 최소값을 건너뛸수 있도록 도와줌
  - 무작위성은 전역 최소값에 다다르지 못하게 할 수 있음
* 학습 스케줄(learning schedule)
  - 매 반복에서 학습률을 결정하는 함수
  - 학습률을 점진적으로 감소(어널링 과정이라고 부름)
  
####4.2.3 미니배치 경사 하강법
* 미니배치 경사 하강법(Mini-batch Gradient Descent)
  - 임의의 작은 샘플 세트에 대해 그래디언트를 계산
  - 행렬 연산에 최적화된 하드웨어(GPU)에서 성능 향상이 큼

###4.3 다항 회귀
###4.4 학습 곡선
###4.5 규제가 있는 선형 모델
####4.5.1 릿지 회귀
####4.5.2 라쏘 회귀
####4.5.3 엘라스틱넷
####4.5.4 조기 종료
###4.6 로지스틱 회귀
####4.6.1 확률 추정
####4.6.2 훈련과 비용 함수
####4.6.3 결정 경계
####4.6.4 소프트맥스 회귀
###4.7 연습문제