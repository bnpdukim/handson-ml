##CHAPTER 4 모델 훈련
###4.1 선형 회귀
* 식 4-1 선형 회귀 모델의 예측
* 식 4-2 선형 회귀 모델의 예측(벡터 형태)
* RMSE를 최소화하는 Θ를 찾아야 함
* RMSE보다 평균 제곱 오차(MSE)를 더 선호함
  - 식 4-3 선형 회귀 모델의 MSE 비용 함수
  - MSE(X,hΘ)를 MSE(Θ)로도 표시 
###4.1.1 정규방정식
* Θ의 값을 얻을 수 있는 수학 공식이 있음
  - 식 4-4 정규방정식
###4.1.2 계산 복잡도
* O(n^2.4) ~ O(n^3)의 계산 복잡도를 가짐(n은 특성 수)
  - 특성수가 두배로 늘어나면 계산시간은 5.3~8배 증가
  - 특성수가 적은 샘플에 적합
* 훈련 세트의 샘플 수에는 선형적으로 증가
  - 메모리 공간이 허락된다면 큰 훈련 세트도 효율적으로 처리

###4.2 경사 하강법
* 경사 하강법(Gradient Descent:GD)은 비용함수를 최소화하기 위해 반복해서 파라미터를 조정
* 그림 4-3 경사 하강법
* 학습률(learning rate) 하이퍼파라미터
  - 학습률이 너무 작으면 시간이 오래 걸림
    - 그림 4-4 학습률이 너무 작을 때
  - 학습률이 너무 크면 큰 값으로 발산되어 해를 찾지 못함
    - 그림 4-5 학습률이 너무 클 때
* 그림 4-6 경사 하강법의 문제점
  - 지역 최소값(local minimum)에 수렴할 수 있음
  - 평탄한 지역에서 멈출수 있음
* 선형 회귀를 위한 MSE 비용 함수는 볼록 함수(convex function)
  - 지역 최소값이 없고 전역 최소값(global minimum)만 존재함
  - 연속된 함수
  - 기울기가 갑자기 변하지 않음
* 그림 4-7 특성 스케일에 따른 경사 하강법
  - 특성들의 스케일링 필요
  - 사이킷런의 StandardScaler를 사용
####4.2.1 배치 경사 하강법
* 편도 함수(partial derivative)
  - Θj가 조금 변경될 때 비용 함수가 얼마나 바뀌는가를 나타내는 함수
  - 식 4-5 비용 함수의 편도함수
  - 식 4-6 비용 함수의 그래디언트 벡터
    - 편도 함수를 사용하여 한꺼번에 계산
* 식 4-7 경사 하강법의 스텝
  - 학습률 Π 사용
  - 그림 4-8 여러 가지 학습률에 대한 경사 하강법
  - 적절한 학습률을 찾으려면 그리드 탐색 사용
    - 수렴하는데 너무 오래 걸리는 것을 막기 위해 반복 횟수 제한 필요
    - 반복 횟수 지정은?
      - 반복 횟수를 크게 지정하고 허용오차보다 작아지면 알고리즘 중지
      
####4.2.2 확률적 경사 하강법
* 배치 경사 하강법의 문제
  - 매 스텝에서 전체 훈련 세트를 사용해 그래디언트를 계산
* 확률적 경사 하강법
  - 매 스텝에서 딱 한 개의 샘플을 무작위로 선택, 그래디언트를 계산
  - 매 반복에서 매우 적은 데이터만 처리하기 때문에 빠름
  - 배치 경사 하강법보다 불안정
  - 불규칙성은 지역 최소값을 건너뛸수 있도록 도와줌
  - 무작위성은 전역 최소값에 다다르지 못하게 할 수 있음
* 학습 스케줄(learning schedule)
  - 매 반복에서 학습률을 결정하는 함수
  - 학습률을 점진적으로 감소(어널링 과정이라고 부름)
  
####4.2.3 미니배치 경사 하강법
* 미니배치 경사 하강법(Mini-batch Gradient Descent)
  - 임의의 작은 샘플 세트에 대해 그래디언트를 계산
  - 행렬 연산에 최적화된 하드웨어(GPU)에서 성능 향상이 큼

###4.3 다항 회귀
* 비선형 데이터를 학습하는데 선형 모델을 사용할 수 있음
  - 각 특성의 거듭제곱을 새로운 특성으로 추가
  - 확정된 특성을 포함한 데이터셋에 선형 모델을 훈련
  - 이런 기법을 다항 회귀(Polynominal Regression)이라 함
  - 그림 4-12 노이즈가 포함된 비선형 데이터셋
  - 사이킷런의 PolynomialFeatures를 사용
  
###4.4 학습 곡선
* 그림 4-14 고차 다항 회귀
  - 일반적으로 어떤 함수로 데이터가 생성됐는지 알 수 없음
    - 4.3절은 2차함수로 만들어서 2차항 회귀를 이용함
  - 얼마나 복잡한 모델을 사용할지 어떻게 결정할 것인가?
  - 과대적합 또는 과소작합을 어떻게 확인할 수 있는가?
* 학습 곡선
  - 훈련 세트에서 크기가 다른 서브 세트를 만들어 모델을 여러 번 훈련
  - 그림 4-15 학습 곡선(선형 회귀)
    - 훈련 세트
      - 하나 혹은 두 개의 샘플이 있을땐 모델이 완벽하게 동작
      - 샘플이 추가됨에 따라 훈련 데이터를 완벽히 학습하는 것은 불가능
    - 검증 데이터
      - 적은 수의 훈련 샘플로 훈련될 때는 제대로 일반화 될 수 없음
      - 샘플이 추가됨에 따라 검증 오차는 감소함
    - 두 곡선이 수평한 구간을 만들고 높은 오차에서 매우 근접해 있음(1.7)
      - 훈련 샘플을 더 추가해도 효과가 없음
      - 더 복잡한 모델을 사용하거나 더 나은 특성을 선택해야 함
  - 그림 4-16 다항 회귀의 학습 곡선(10차)
    - 훈련 데이터의 오차가 선형 회귀 모델보다 낮음
    - 두 곡선 사이에 공간이 있음
      - 과대적합
      - 더 큰 훈련 세트를 사용하면 점점 가까워짐
    
###4.5 규제가 있는 선형 모델
* 과대적합을 감소시키는 좋은 방법은 모델을 규제하는 것
* 선형 회귀 모델에서는 보통 모델의 가중치를 제한함으로써 규제를 가함
  - 릿지 회귀, 라쏘 회귀, 엘라스틱넷

####4.5.1 릿지 회귀
* 규제가 추가된 선형 회귀 버전
  - 규제항이 비용함수에 추가됨
    - a=0이면 선형회귀
    - a가 크면 평균을 지나는 수평선 
  - 식 4-8 릿지 회귀의 비용 함수
* 규제가 있는 모델들은 데이터 스케일링 필수(e.g. StandardScaler)
* 그림 4-17 릿지 회귀

####4.5.2 라쏘 회귀
* 규제항으로 l1노름을 사용함
  - 식 4-10 라쏘 회귀의 비용 함수
* 그림 4-18 라쏘 회귀
* 그림 4-19 라쏘 및 릿지 규제
####4.5.3 엘라스틱넷
* 릿지 회귀와 라쏘 회귀를 절충한 모델
  - 혼합 비율 r을 사용해 조율
* 규제가 없는 선형 회귀, 릿지, 라쏘, 엘라스틱넷을 언제 사용해야 할까?
  - 릿지가 기본
  - 실제로 쓰이는 특성이 소수라고 의심되면 라쏘나 엘라스틱넷이 나음
  - 특성 수가 훈련 샘플 수보다 많거나 특성 몇 개가 강하게 연관되어 있을 때는 엘라스틱넷 선호  
####4.5.4 조기 종료
* 검증 에러가 최소값에 도달하면 바로 훈련을 중지
  - 그림 4-20 조기 종료 규제
  - 검증 에러가 일정 시간 동안 최소값보다 클 때 학습을 멈추고 검증 에러가 최소였을때의 모델 파라미터로 되돌림
###4.6 로지스틱 회귀
* 샘플이 특정 클래스에 속할 확률을 추정하는데 널리 사용됨
####4.6.1 확률 추정
* 식 4-13 로지스틱 회귀 모델의 확률 추정(벡터 표현식)
  - 시그모이드 함수(sigmoid function)
    - 식 4-14 로지스틱 함수
    - 그림 4-21 로지스틱 함수
  -식 4-15 로지스틱 회귀 모델 예측
####4.6.2 훈련과 비용 함수
* 훈련의 목적
  - 양성 샘플(y=1)에 대해서는 높은 확률 추정
  - 음성 샘플(y=0)에 대해서는 낮은 확률을 추정
  - 식 4-16 하나의 훈련 샘플에 대한 비용 함수
  - 식 4-17 로지스틱 회귀의 비용 함수(로그손실)
  - 비용함수는 볼록 함수이므로 경사하강법이 전역 최소값을 찾는 것을 보장
    - j번째 모델 파라미터 Θj에 대해 편미분
      - 식 4-18 로지스틱 비용 함수의 편도함수
####4.6.3 결정 경계
* 꽃잎 너비 기반으로 Iris-Verisicolor 종 감지 분류기
* 로지스틱 회귀 모델도 l1,l2 페널티를 사용하여 규제할 수 있음
  - 사이킷런은 l2 페널티를 기본으로 함
####4.6.4 소프트맥스 회귀
###4.7 연습문제