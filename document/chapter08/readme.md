##CHAPTER 8 차원 축소
* 차원의 저주(curse of dimensionality)
  - 훈련을 느리게 함
  - 좋은 솔루션을 찾기가 어려움
* 특성 수를 줄여서 불가능한 문제를 가능한 범위로 변경할 수 있음
* 차원 축소에 사용되는 접근 방법
  - 투영(projection)
  - 매니폴드 학습(Manifold Learning)

###8.1 차원의 저주
* 고차원 초입방제에 있는 대다수의 점은 경계와 매우 가까이 있음
  - 2차원의 단위 면적안에 있는 점을 무작위로 선택시 경계선에서 0.001이내에 위치할 가능성은 0.4%
  - 10000차원의 경우 99.9999999% 보다 커짐
* 단위 면적에서 임의의 두 점 사이의 거리 평균은 0.52
  - 3차원 큐브에서는 0.66
  - 1000000차원차원의 평균거리는 약 428.25
  - 고차원의 데이터셋이 매우 희박한 상태일 수 있음
* 세트의 차원이 클수록 과대적합 위험이 커짐
* 차원의 저주 해결책
  - 훈련 샘플의 밀도가 충분히 높아질 때까지 훈련 세트의 크기를 키우기
    - 특성수가 100개일 경우 평균 0.1 이내에 위치시키려면 우주에 있는 원자수를 합친것 보다 더 모아야함  

###8.2 차원 축소를 위한 접근 방법

####8.2.1 투영
* 고차원 공간 안의 저차원 부분 공간(subspace)에 놓여 있음
  - 그림 8-2 2차원에 가깝게 배치된 3차원 데이터셋
    - 그림 8-3 투영하여 만들어진 새로운 2D 데이터셋
      - 모든 훈련 샘플을 부분 공간에 수직으로 투영하면 2D 데이터셋을 얻을 수 있음
      - 각 축은 새로운 특성 z1, z2에 대응
* 스위스 롤(Swiss roll)
  - 돌돌 말은 형태의 케이크
  - 롤케이크
  - 그림 8-4 스위스 롤 데이터셋
    - 부분 공간이 뒤틀리거나 휘어 있기도 함
    - 그림 8-5 평면에 그냥 투영시켜서 뭉개진 것과 스위스 롤을 펼쳐 놓은것
      - 투영이 언제나 최선의 방법은 아님

####8.2.2 매니폴드 학습
* 2D 매니폴드
  - 고차원 공간에서 휘어지거나 뒤틀린 2D 모양
  - 스위스 롤은 2D 매니폴드의 한 예
  - 국부적으로 d차원 초평면으로 보일 수 있는 n차원 공간의 일부(d<n)
    - 스위스 롤의 경우 d=2, n=3
* 매니폴드 학습(Manifold Learning)
  - 매니폴드 가정(minifold assumption) 또는 매니폴드 가설(manifold hypothesis)에 근거함
    - 고차원 데이터셋이 더 낮은 저차원 매니폴드에 가깝게 놓여 있음
  - 무작위로 생성된 이미지보다 숫자 이미지를 만들때의 자유도가 낮음
    - 데이터셋을 저차원의 매니폴드로 압축할 수 있도록 도와줌
  - 암묵적 가정
    - 저차원의 매니폴드 공간에 표현되면 더 간단해 짐
    - 항상 유효하지 않음
      - 그림 8-6 저차원에서 항상 간단하지 않은 결정 경계
* 훈련 세트의 차원을 감소시키면 훈련 속도는 빨라지지만 항상 더 낫거나 간단한 솔루션이 되는 것은 아님
  - 전적으로 데이터셋에 의존적임

###8.3 PCA
* 

####8.3.1 분산 보존

####8.3.2 주성분

####8.3.3 d차원으로 투영하기

####8.3.4 사이킷런 사용하기

####8.3.5 설명된 분산의 비율

####8.3.6 설명된 분산의 비율

####8.3.7 압축을 위한 PCA

####8.3.8 점진적 PCA

####8.3.9 랜덤 PCA

###8.4 커널 PCA

####8.4.1 커널 선택과 하이퍼파라미터 튜닝

###8.5 LLE

###8.6 다른 차원 축소 기법

###8.7 연습 문제